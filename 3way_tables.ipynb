{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spiaggesi/anaconda3/envs/tf2/lib/python3.6/site-packages/sklearn/utils/deprecation.py:143: FutureWarning: The sklearn.metrics.ranking module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.metrics. Anything that cannot be imported from sklearn.metrics is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASETS =  ['contact-high-school', 'contact-primary-school']\n",
    "SG = 'cbow'\n",
    "HASSE_LIST = ['uniform', 'counts', 'NObias', 'LOexp']\n",
    "WORK_FOLDER = './'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\"harm_mean\", \"geom_mean\", \"arith_mean\", \"adamic_adar\", \"simplex_PA\", \"WPKatz\", \"WPPR\", \"logreg_supervised\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct reconstruction/prediction test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for DATASET in DATASETS:\n",
    "    \n",
    "    hyperedges_path = WORK_FOLDER + 'processed-output/hyperedges/%s/'%(DATASET)\n",
    "\n",
    "    save_path = WORK_FOLDER + 'processed-output/tables/%s/'%(DATASET)\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "    load_path = WORK_FOLDER + '3way-metrics-data/'\n",
    "\n",
    "    #Load Simplices Data\n",
    "    _, _, data_train, _ = make_train_test_data(DATASET)\n",
    "    proj_g = nx.Graph([tuple(s) for s in data_train if len(s)==2])\n",
    "    nodes_train = sorted(nx.connected_components(proj_g), key=len, reverse=True)[0]\n",
    "\n",
    "    ##############\n",
    "    \n",
    "    #reconstruction sets\n",
    "    open_train = np.load(load_path + '%s-open-tris-0-80.npz'% DATASET)\n",
    "    triangles_train = open_train[:,:3].astype(str)\n",
    "    \n",
    "    #keep triples in the largest component\n",
    "    idx_train = [i for i, triangle in enumerate(triangles_train) if set(triangle).issubset(nodes_train)]\n",
    "\n",
    "    positive_ex = set(np.load(hyperedges_path+'%s_pos_%s_%dstring.npz'%('reconstruction', 'all', 3),\\\n",
    "                              allow_pickle=True)['arr_0'])\n",
    "\n",
    "    y_train = np.array([','.join(map(str, sorted(map(int, tris)))) \\\n",
    "                               in positive_ex for tris in triangles_train[idx_train]]).astype(int)\n",
    "\n",
    "    open_train[idx_train, -1] = y_train\n",
    "    np.savez_compressed(save_path+'open-tris-0-80.npz', open_train[idx_train])\n",
    "\n",
    "    ##############\n",
    "    \n",
    "    #prediction sets\n",
    "    open_test = np.load(load_path + '%s-open-tris-80-100.npz'% DATASET)\n",
    "    triangles_test = open_test[:,:3].astype(str)\n",
    "    \n",
    "    #keep triples in the largest component\n",
    "    idx_test = [i for i, triangle in enumerate(triangles_test) if set(triangle).issubset(nodes_train)]\n",
    "\n",
    "    positive_ex = np.load(hyperedges_path+'%s_pos_%s_%dstring.npz'%('prediction', 'all', 3),\\\n",
    "                          allow_pickle=True)['arr_0']\n",
    "    positive_bounds = np.load(hyperedges_path+'%s_pos_%s_%dbounds.npz'%('prediction', 'all', 3),\\\n",
    "                              allow_pickle=True)['arr_0']\n",
    "\n",
    "    positive_ex = set(positive_ex[positive_bounds==3])\n",
    "\n",
    "    y_test = np.array([','.join(map(str, sorted(map(int, tris)))) \\\n",
    "                               in positive_ex for tris in triangles_test[idx_test]]).astype(int)\n",
    "\n",
    "    open_test[idx_test, -1] = y_test\n",
    "    np.savez_compressed(save_path+'open-tris-80-100.npz', open_test[idx_test])\n",
    "    \n",
    "    ################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect metrics from Benson et al., ref [9] of the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for DATASET in DATASETS:\n",
    "    \n",
    "    save_path = WORK_FOLDER + 'processed-output/tables/%s/'%(DATASET)\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "    load_path = WORK_FOLDER + '3way-metrics-data/'\n",
    "\n",
    "    #Load Simplices Data\n",
    "    _, _, data_train, _ = make_train_test_data(DATASET)\n",
    "    proj_g = nx.Graph([tuple(s) for s in data_train if len(s)==2])\n",
    "    nodes_train = sorted(nx.connected_components(proj_g), key=len, reverse=True)[0]\n",
    "\n",
    "    ##############\n",
    "    \n",
    "    #reconstruction metrics\n",
    "    open_train = np.load(load_path + '%s-open-tris-0-80.npz'% DATASET)\n",
    "    triangles_train = open_train[:,:3].astype(str)\n",
    "    \n",
    "    #keep triples in the largest component\n",
    "    idx_train = [i for i, triangle in enumerate(triangles_train) if set(triangle).issubset(nodes_train)]\n",
    "    \n",
    "    for m in metrics:\n",
    "        try:\n",
    "            y_pred = np.load(load_path+'%s-open-tris-scores-0-80-%s.npz'%(DATASET, m))\n",
    "            np.savez_compressed(save_path+'open-tris-%s-0-80.npz'%m, y_pred[idx_train])\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "\n",
    "\n",
    "    ##############\n",
    "    \n",
    "    #prediction metrics\n",
    "    open_test = np.load(load_path + '%s-open-tris-80-100.npz'% DATASET)\n",
    "    triangles_test = open_test[:,:3].astype(str)\n",
    "    \n",
    "    #keep triples in the largest component\n",
    "    idx_test = [i for i, triangle in enumerate(triangles_test) if set(triangle).issubset(nodes_train)]\n",
    "\n",
    "    for m in metrics:\n",
    "        y_pred = np.load(load_path+'%s-open-tris-scores-80-100-%s.npz'%(DATASET, m))\n",
    "        np.savez_compressed(save_path+'open-tris-%s-80-100.npz'%m, y_pred[idx_test])\n",
    "        \n",
    "    ##############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Reconstruction Scores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = 1.\n",
    "N = 10\n",
    "WALKLEN = 80\n",
    "SEED = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "embdim_list = [8, 16, 32, 64, 128, 256, 512, 1024]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for DATASET in DATASETS:\n",
    "\n",
    "    load_path = WORK_FOLDER + 'processed-output/tables/%s/'%(DATASET)\n",
    "\n",
    "    _, _, data_train, _ = make_train_test_data(DATASET)\n",
    "    proj_g = nx.Graph([tuple(s) for s in data_train if len(s)==2])\n",
    "    nodes_train = sorted(nx.connected_components(proj_g), key=len, reverse=True)[0]\n",
    "\n",
    "    open_test = np.load(load_path + 'open-tris-0-80.npz')['arr_0']\n",
    "    triangles_test = open_test[:,:3].astype(str)\n",
    "    y_test = open_test[:,-1]\n",
    "\n",
    "    assert(np.unique(y_test).shape[0]>1)\n",
    "    \n",
    "    for HASSE_TYPE in HASSE_LIST:\n",
    "\n",
    "        for max_order in range(1, MAX_ORDER+1):\n",
    "            for EMBDIM in embdim_list:\n",
    "\n",
    "                PARAMS = '%s_%s_%s_%s' %\\\n",
    "                                ( 'dim'+str(EMBDIM), 'n'+str(N), 'p'+str(P), 'walklen'+str(WALKLEN))\n",
    "\n",
    "                load_path = WORK_FOLDER + 'processed-output/embeddings/%s/%s/'%(DATASET, PARAMS)\n",
    "\n",
    "                save_path = WORK_FOLDER + 'processed-output/tables/%s/%s/'%(DATASET, PARAMS)\n",
    "                os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "                if os.path.isdir(load_path):\n",
    "\n",
    "                    #Load Embeddings\n",
    "                    with open(load_path+'s2vembs_%s_%s_maxorder%s.%s.pkl'%\\\n",
    "                                (SG, HASSE_TYPE, max_order, SEED), 'rb') as fh:\n",
    "                        model_wv = pkl.load(fh)\n",
    "\n",
    "                    #node embedding\n",
    "                    tf_arrays = map(lambda a: [(model_wv[h], model_wv[k]) for h,k in combinations(a,2)],\n",
    "                                            triangles_test)\n",
    "\n",
    "                    X_test = np.array(list(map(lambda x: np.mean([a*b for a,b in x], axis=0), tf_arrays)))\n",
    "                    y_pred = X_test.sum(axis=1)\n",
    "                    np.savez_compressed(save_path+'open-tris-hadamard-sim-0simplex-0-80-%s-%s-%s-maxorder%s.%s.npz'%\\\n",
    "                                            (SG, 's2v', HASSE_TYPE, max_order, SEED), y_pred)\n",
    "\n",
    "                    #edge embedding\n",
    "                    tf_arrays = map(lambda a: [(model_wv[h], model_wv[k]) for h,k in\n",
    "                            combinations([','.join(map(str, sorted(map(int, edge)))) for edge in combinations(a,2)], 2)],\n",
    "                            triangles_test)\n",
    "\n",
    "                    X_test = np.array(list(map(lambda x: np.mean([a*b for a,b in x], axis=0), tf_arrays)))\n",
    "                    y_pred = X_test.sum(axis=1)\n",
    "                    np.savez_compressed(save_path+'open-tris-hadamard-sim-1simplex-0-80-%s-%s-%s-maxorder%s.%s.npz'%\\\n",
    "                                            (SG, 's2v', HASSE_TYPE, max_order, SEED), y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconstruction Search and Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contact-high-school\n",
      "\n",
      "harm_mean =  85.5 ± 1.5\n",
      "geom_mean =  85.8 ± 1.1\n",
      "arith_mean =  78.8 ± 1.1\n",
      "adamic_adar =  56.5 ± 1.4\n",
      "simplex_PA =  58.3 ± 1.4\n",
      "WPKatz =  78.6 ± 1.1\n",
      "WPPR =  76.9 ± 1.4\n",
      "\n",
      "H1: uniform\n",
      "s0 =  56.5 ± 1.9 (64dims)\n",
      "s1 =  52.3 ± 1.6 (8dims)\n",
      "H2: uniform\n",
      "s0 =  55.2 ± 2.1 (8dims)\n",
      "s1 =  99.5 ± 0.1 (512dims)\n",
      "\n",
      "H1: counts\n",
      "s0 =  79.8 ± 1.1 (128dims)\n",
      "s1 =  84.4 ± 1.0 (8dims)\n",
      "H2: counts\n",
      "s0 =  56.6 ± 1.1 (128dims)\n",
      "s1 =  91.3 ± 0.9 (128dims)\n",
      "\n",
      "H1: NObias\n",
      "s0 =  79.5 ± 1.0 (128dims)\n",
      "s1 =  84.4 ± 0.9 (8dims)\n",
      "H2: NObias\n",
      "s0 =  73.0 ± 1.1 (512dims)\n",
      "s1 =  89.1 ± 0.7 (256dims)\n",
      "\n",
      "H1: LOexp\n",
      "s0 =  81.7 ± 2.2 (16dims)\n",
      "s1 =  89.5 ± 0.9 (8dims)\n",
      "H2: LOexp\n",
      "s0 =  84.4 ± 1.6 (16dims)\n",
      "s1 =  91.9 ± 0.8 (8dims)\n",
      "\n",
      "contact-primary-school\n",
      "\n",
      "harm_mean =  88.2 ± 0.7\n",
      "geom_mean =  88.9 ± 0.6\n",
      "arith_mean =  83.9 ± 0.5\n",
      "adamic_adar =  63.6 ± 0.9\n",
      "simplex_PA =  51.6 ± 0.6\n",
      "WPKatz =  83.9 ± 0.5\n",
      "WPPR =  83.5 ± 0.4\n",
      "\n",
      "H1: uniform\n",
      "s0 =  64.9 ± 0.6 (64dims)\n",
      "s1 =  58.0 ± 0.5 (8dims)\n",
      "H2: uniform\n",
      "s0 =  64.4 ± 0.6 (8dims)\n",
      "s1 =  99.5 ± 0.1 (512dims)\n",
      "\n",
      "H1: counts\n",
      "s0 =  82.4 ± 0.6 (16dims)\n",
      "s1 =  88.5 ± 0.6 (16dims)\n",
      "H2: counts\n",
      "s0 =  74.5 ± 0.6 (64dims)\n",
      "s1 =  93.6 ± 0.5 (256dims)\n",
      "\n",
      "H1: NObias\n",
      "s0 =  82.4 ± 0.5 (16dims)\n",
      "s1 =  88.5 ± 0.6 (16dims)\n",
      "H2: NObias\n",
      "s0 =  81.1 ± 0.5 (128dims)\n",
      "s1 =  93.1 ± 0.4 (512dims)\n",
      "\n",
      "H1: LOexp\n",
      "s0 =  82.4 ± 0.7 (16dims)\n",
      "s1 =  89.5 ± 0.5 (8dims)\n",
      "H2: LOexp\n",
      "s0 =  84.5 ± 0.6 (16dims)\n",
      "s1 =  91.4 ± 0.4 (8dims)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for DATASET in DATASETS:\n",
    "    \n",
    "    load_path = WORK_FOLDER + 'processed-output/tables/%s/'%(DATASET)\n",
    "\n",
    "    y_test = np.load(load_path+'open-tris-0-80.npz')['arr_0'][:,-1]\n",
    "    random_baseline = y_test.sum()/len(y_test)\n",
    "    \n",
    "    print(DATASET)\n",
    "    print()\n",
    "\n",
    "    scores_baseline = []\n",
    "    for m in metrics[:-1]:\n",
    "        y_pred = np.load(load_path+'open-tris-%s-0-80.npz'%m)['arr_0']\n",
    "        scores_baseline.append((classification_score_from_y(y_test, y_pred), m))\n",
    "\n",
    "    for s,m in scores_baseline:\n",
    "        print(m + ' = ', round(s[0]*100,1) , u\"\\u00B1\", round(s[1]*100, 1) )\n",
    "    print()\n",
    "\n",
    "    params_folders = glob.glob(load_path+'dim*')\n",
    "    \n",
    "    for HASSE_TYPE in HASSE_LIST:\n",
    "\n",
    "        for max_order in range(1, MAX_ORDER):\n",
    "\n",
    "            print('H'+str(max_order)+':', HASSE_TYPE)\n",
    "\n",
    "            scores_simplex0 = []\n",
    "            scores_simplex1 = []\n",
    "            for f in params_folders:\n",
    "                PARAMS = f.split('/')[-1]\n",
    "                EMBDIM = PARAMS.split('_')[0].replace('dim', '')\n",
    "\n",
    "                y_pred = np.load(f + '/open-tris-hadamard-sim-0simplex-0-80-%s-%s-%s-maxorder%s.%s.npz'%\\\n",
    "                                (SG, 's2v', HASSE_TYPE, max_order, 0))['arr_0']\n",
    "                scores_simplex0.append((classification_score_from_y(y_test, y_pred), EMBDIM+'dims'))\n",
    "\n",
    "                y_pred = np.load(f + '/open-tris-hadamard-sim-1simplex-0-80-%s-%s-%s-maxorder%s.%s.npz'%\\\n",
    "                                (SG, 's2v', HASSE_TYPE, max_order, 0))['arr_0']\n",
    "                scores_simplex1.append((classification_score_from_y(y_test, y_pred), EMBDIM+'dims'))\n",
    "\n",
    "            scores_simplex0 = sorted(scores_simplex0, reverse=True)\n",
    "            scores_simplex1 = sorted(scores_simplex1, reverse=True)\n",
    "            print('s0 = ', round(scores_simplex0[0][0][0]*100,1), u\"\\u00B1\", round(scores_simplex0[0][0][1]*100,1), '('+scores_simplex0[0][1]+')') \n",
    "            print('s1 = ',round(scores_simplex1[0][0][0]*100,1), u\"\\u00B1\", round(scores_simplex1[0][0][1]*100,1), '('+scores_simplex1[0][1]+')') \n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Prediction Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = 1.\n",
    "N = 10\n",
    "WALKLEN = 80\n",
    "SEED = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "embdim_list = [8, 16, 32, 64, 128, 256, 512, 1024]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for DATASET in DATASETS:\n",
    "\n",
    "    load_path = WORK_FOLDER + 'processed-output/tables/%s/'%(DATASET)\n",
    "\n",
    "    _, _, data_train, _ = make_train_test_data(DATASET)\n",
    "    proj_g = nx.Graph([tuple(s) for s in data_train if len(s)==2])\n",
    "    nodes_train = sorted(nx.connected_components(proj_g), key=len, reverse=True)[0]\n",
    "\n",
    "    open_test = np.load(load_path + 'open-tris-80-100.npz')['arr_0']\n",
    "    triangles_test = open_test[:,:3].astype(str)\n",
    "    y_test = open_test[:,-1]\n",
    "\n",
    "    assert(np.unique(y_test).shape[0]>1)\n",
    "    \n",
    "    for HASSE_TYPE in HASSE_LIST:\n",
    "        \n",
    "        for max_order in range(1, MAX_ORDER+1):\n",
    "            for EMBDIM in embdim_list:\n",
    "\n",
    "                PARAMS = '%s_%s_%s_%s' %\\\n",
    "                                ( 'dim'+str(EMBDIM), 'n'+str(N), 'p'+str(P), 'walklen'+str(WALKLEN))\n",
    "\n",
    "                load_path = WORK_FOLDER + 'processed-output/embeddings/%s/%s/'%(DATASET, PARAMS)\n",
    "\n",
    "                save_path = WORK_FOLDER + 'processed-output/tables/%s/%s/'%(DATASET, PARAMS)\n",
    "                os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "                if os.path.isdir(load_path):\n",
    "\n",
    "                    #Load Embeddings\n",
    "                    with open(load_path+'s2vembs_%s_%s_maxorder%s.%s.pkl'%\\\n",
    "                                (SG, HASSE_TYPE, max_order, SEED), 'rb') as fh:\n",
    "                        model_wv = pkl.load(fh)\n",
    "\n",
    "                    #node embedding\n",
    "                    tf_arrays = map(lambda a: [(model_wv[h], model_wv[k]) for h,k in combinations(a,2)],\n",
    "                                            triangles_test)\n",
    "\n",
    "                    X_test = np.array(list(map(lambda x: np.mean([a*b for a,b in x], axis=0), tf_arrays)))\n",
    "                    y_pred = X_test.sum(axis=1)\n",
    "                    np.savez_compressed(save_path+'open-tris-hadamard-sim-0simplex-80-100-%s-%s-%s-maxorder%s.%s.npz'%\\\n",
    "                                            (SG, 's2v', HASSE_TYPE, max_order, SEED), y_pred)\n",
    "\n",
    "                    #edge embedding\n",
    "                    tf_arrays = map(lambda a: [(model_wv[h], model_wv[k]) for h,k in\n",
    "                            combinations([','.join(map(str, sorted(map(int, edge)))) for edge in combinations(a,2)], 2)],\n",
    "                            triangles_test)\n",
    "\n",
    "                    X_test = np.array(list(map(lambda x: np.mean([a*b for a,b in x], axis=0), tf_arrays)))\n",
    "                    y_pred = X_test.sum(axis=1)\n",
    "                    np.savez_compressed(save_path+'open-tris-hadamard-sim-1simplex-80-100-%s-%s-%s-maxorder%s.%s.npz'%\\\n",
    "                                            (SG, 's2v', HASSE_TYPE, max_order, SEED), y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction Search and Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contact-high-school\n",
      "\n",
      "harm_mean =  71.4 ± 4.3\n",
      "geom_mean =  73.1 ± 3.8\n",
      "arith_mean =  69.3 ± 3.6\n",
      "adamic_adar =  64.8 ± 5.6\n",
      "simplex_PA =  54.2 ± 6.0\n",
      "WPKatz =  69.3 ± 3.7\n",
      "WPPR =  69.8 ± 3.9\n",
      "logreg_supervised =  68.7 ± 3.1\n",
      "\n",
      "H1: uniform\n",
      "s0 =  62.6 ± 5.8 (64dims)\n",
      "s1 =  49.8 ± 7.4 (8dims)\n",
      "H2: uniform\n",
      "s0 =  60.7 ± 3.7 (16dims)\n",
      "s1 =  68.3 ± 4.7 (16dims)\n",
      "\n",
      "H1: counts\n",
      "s0 =  74.8 ± 3.3 (32dims)\n",
      "s1 =  72.8 ± 3.6 (8dims)\n",
      "H2: counts\n",
      "s0 =  65.6 ± 3.5 (256dims)\n",
      "s1 =  73.1 ± 3.5 (8dims)\n",
      "\n",
      "H1: NObias\n",
      "s0 =  74.1 ± 3.4 (32dims)\n",
      "s1 =  73.1 ± 3.4 (8dims)\n",
      "H2: NObias\n",
      "s0 =  70.5 ± 3.4 (64dims)\n",
      "s1 =  73.4 ± 3.8 (8dims)\n",
      "\n",
      "H1: LOexp\n",
      "s0 =  69.9 ± 2.7 (16dims)\n",
      "s1 =  65.1 ± 5.1 (8dims)\n",
      "H2: LOexp\n",
      "s0 =  70.6 ± 3.3 (32dims)\n",
      "s1 =  65.6 ± 5.0 (8dims)\n",
      "\n",
      "contact-primary-school\n",
      "\n",
      "harm_mean =  80.9 ± 1.3\n",
      "geom_mean =  82.3 ± 1.4\n",
      "arith_mean =  76.6 ± 1.8\n",
      "adamic_adar =  65.5 ± 1.4\n",
      "simplex_PA =  48.8 ± 1.9\n",
      "WPKatz =  77.5 ± 1.7\n",
      "WPPR =  79.9 ± 1.4\n",
      "logreg_supervised =  80.7 ± 1.0\n",
      "\n",
      "H1: uniform\n",
      "s0 =  67.9 ± 1.6 (64dims)\n",
      "s1 =  59.0 ± 1.7 (8dims)\n",
      "H2: uniform\n",
      "s0 =  65.6 ± 2.5 (8dims)\n",
      "s1 =  79.5 ± 1.3 (8dims)\n",
      "\n",
      "H1: counts\n",
      "s0 =  80.6 ± 1.3 (64dims)\n",
      "s1 =  81.9 ± 1.5 (8dims)\n",
      "H2: counts\n",
      "s0 =  74.0 ± 1.4 (64dims)\n",
      "s1 =  81.5 ± 1.3 (16dims)\n",
      "\n",
      "H1: NObias\n",
      "s0 =  80.4 ± 1.1 (16dims)\n",
      "s1 =  81.9 ± 1.5 (8dims)\n",
      "H2: NObias\n",
      "s0 =  79.9 ± 1.3 (64dims)\n",
      "s1 =  81.8 ± 1.4 (8dims)\n",
      "\n",
      "H1: LOexp\n",
      "s0 =  78.0 ± 1.3 (64dims)\n",
      "s1 =  75.3 ± 1.5 (8dims)\n",
      "H2: LOexp\n",
      "s0 =  79.2 ± 1.2 (128dims)\n",
      "s1 =  75.5 ± 1.3 (8dims)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for DATASET in DATASETS:\n",
    "\n",
    "    load_path = WORK_FOLDER + 'processed-output/tables/%s/'%(DATASET)\n",
    "\n",
    "    y_test = np.load(load_path+'open-tris-80-100.npz')['arr_0'][:,-1]\n",
    "    random_baseline = y_test.sum()/len(y_test)\n",
    "    \n",
    "    print(DATASET)\n",
    "    print()\n",
    "\n",
    "    scores_baseline = []\n",
    "    for m in metrics:\n",
    "        y_pred = np.load(load_path+'open-tris-%s-80-100.npz'%m)['arr_0']\n",
    "        scores_baseline.append((classification_score_from_y(y_test, y_pred), m))\n",
    "\n",
    "    for s,m in scores_baseline:\n",
    "        print(m + ' = ', round(s[0]*100,1) , u\"\\u00B1\", round(s[1]*100, 1) )\n",
    "    print()\n",
    "\n",
    "    params_folders = glob.glob(load_path+'dim*')\n",
    "    \n",
    "    for HASSE_TYPE in HASSE_LIST:\n",
    "\n",
    "        for max_order in range(1, MAX_ORDER):\n",
    "\n",
    "            print('H'+str(max_order)+':', HASSE_TYPE)\n",
    "\n",
    "            scores_simplex0 = []\n",
    "            scores_simplex1 = []\n",
    "            for f in params_folders:\n",
    "                PARAMS = f.split('/')[-1]\n",
    "                EMBDIM = PARAMS.split('_')[0].replace('dim', '')\n",
    "\n",
    "                y_pred = np.load(f + '/open-tris-hadamard-sim-0simplex-80-100-%s-%s-%s-maxorder%s.%s.npz'%\\\n",
    "                                (SG, 's2v', HASSE_TYPE, max_order, 0))['arr_0']\n",
    "                scores_simplex0.append((classification_score_from_y(y_test, y_pred), EMBDIM+'dims'))\n",
    "\n",
    "                y_pred = np.load(f + '/open-tris-hadamard-sim-1simplex-80-100-%s-%s-%s-maxorder%s.%s.npz'%\\\n",
    "                                (SG, 's2v', HASSE_TYPE, max_order, 0))['arr_0']\n",
    "                scores_simplex1.append((classification_score_from_y(y_test, y_pred), EMBDIM+'dims'))\n",
    "\n",
    "            scores_simplex0 = sorted(scores_simplex0, reverse=True)\n",
    "            scores_simplex1 = sorted(scores_simplex1, reverse=True)\n",
    "            print('s0 = ', round(scores_simplex0[0][0][0]*100,1), u\"\\u00B1\", round(scores_simplex0[0][0][1]*100,1), '('+scores_simplex0[0][1]+')') \n",
    "            print('s1 = ',round(scores_simplex1[0][0][0]*100,1), u\"\\u00B1\", round(scores_simplex1[0][0][1]*100,1), '('+scores_simplex1[0][1]+')') \n",
    "        print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
