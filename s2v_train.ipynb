{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import networkx as nx\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASETS = ['contact-high-school', 'contact-primary-school']\n",
    "SG = 'cbow'\n",
    "HASSE_LIST = ['uniform', 'counts', 'NObias', 'LOexp'] \n",
    "WORK_FOLDER = './'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Hasse Diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simplex2hasse import simplex2hasse_HOexponential, simplex2hasse_LOexponential, simplex2hasse_counts, simplex2hasse_uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for DATASET in DATASETS: \n",
    "\n",
    "    #load train simplices data\n",
    "    cliques_train, _, _, _ = make_train_test_data(DATASET)\n",
    "\n",
    "    save_path = WORK_FOLDER + 'processed-output/walks/%s/'%(DATASET)\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    \n",
    "    #loop over different weighting\n",
    "    for HASSE_TYPE in HASSE_LIST:\n",
    "\n",
    "        #loop over interaction orders\n",
    "        for max_order in range(1, MAX_ORDER+1):\n",
    "\n",
    "            # build Hasse Diag\n",
    "            if HASSE_TYPE=='uniform':\n",
    "                g_hasse = simplex2hasse_uniform(list(set(cliques_train)), max_order=max_order)\n",
    "            if HASSE_TYPE=='counts' or HASSE_TYPE=='NObias':\n",
    "                g_hasse = simplex2hasse_counts(cliques_train, max_order=max_order)\n",
    "            if HASSE_TYPE=='HOexp':\n",
    "                g_hasse = simplex2hasse_HOexponential(list(set(cliques_train)), max_order=max_order)\n",
    "            if HASSE_TYPE=='LOexp':\n",
    "                g_hasse = simplex2hasse_LOexponential(list(set(cliques_train)), max_order=max_order)\n",
    "\n",
    "            # convert to convenient format\n",
    "            g_hasse = s2vhasse_to_n2vformat(g_hasse)\n",
    "\n",
    "            # compute weights without bias towards lower-upper orders\n",
    "            if HASSE_TYPE=='NObias':\n",
    "                g_hasse = unbias_weights_n2vformat(g_hasse)\n",
    "\n",
    "            node_name = np.array(list(g_hasse.nodes()))\n",
    "            node_index = {node:index for index, node in enumerate(node_name)}\n",
    "            g_hasse = nx.relabel_nodes(g_hasse, node_index)\n",
    "\n",
    "            #save Hasse Diag\n",
    "            np.savez_compressed(save_path + 'hasse_%s_maxorder%d.nodename.npz'%(HASSE_TYPE, max_order), node_name)\n",
    "            nx.write_weighted_edgelist(g_hasse, \n",
    "                              save_path + 'hasse_%s_maxorder%d.edgelist.gz'%(HASSE_TYPE, max_order))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample Random Walks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snap_node2vec import snap_node2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters for random walk sampling\n",
    "P = 1.\n",
    "N = 10\n",
    "WALKLEN = 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for DATASET in DATASETS: \n",
    "\n",
    "    load_path = WORK_FOLDER + 'processed-output/walks/%s/'%(DATASET)\n",
    "    save_path = WORK_FOLDER + 'processed-output/walks/%s/'%(DATASET)\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    \n",
    "    #loop over different weighting\n",
    "    for HASSE_TYPE in HASSE_LIST:\n",
    "\n",
    "        #loop over interaction orders\n",
    "        for max_order in range(1, MAX_ORDER+1):\n",
    "\n",
    "            g_hasse = nx.read_weighted_edgelist(load_path + 'hasse_%s_maxorder%d.edgelist.gz'%(HASSE_TYPE, max_order), \n",
    "                                                create_using=nx.DiGraph)\n",
    "\n",
    "            nx.relabel_nodes(g_hasse, {i: int(i) for i in g_hasse.nodes()}, copy=False)\n",
    "\n",
    "            #sample random walks\n",
    "            node2vec = snap_node2vec(d=2, max_iter=1, walk_len=WALKLEN, num_walks=N, con_size=5, ret_p=P, inout_p=1.)\n",
    "            _ = node2vec.save_random_walks(g_hasse, edge_f = None, is_weighted=True, \n",
    "                          no_python=True, directed=True, save_directory=save_path, \n",
    "                          file_name='%s_walks_simplex2vec_%s_maxorder%d.txt'%('n%s_p%s'%(str(N),str(P)), HASSE_TYPE, max_order),\n",
    "                          compress=True)\n",
    "            del node2vec\n",
    "            g_hasse.clear()\n",
    "            \n",
    "            print('DONE:', DATASET, HASSE_TYPE, max_order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train simplex2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = 1.\n",
    "N = 10\n",
    "WALKLEN = 80\n",
    "SEED = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embdim_list = [8, 16, 32, 64, 128, 256, 512, 1024]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for DATASET in DATASETS: \n",
    "\n",
    "    load_path = WORK_FOLDER + 'processed-output/walks/%s/'%(DATASET)\n",
    "    save_path = WORK_FOLDER + 'processed-output/embeddings/%s/'%(DATASET)\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    \n",
    "    #loop over different weighting\n",
    "    for HASSE_TYPE in HASSE_LIST:\n",
    "        \n",
    "        #loop over interaction orders\n",
    "        for max_order in range(1, MAX_ORDER+1):\n",
    "\n",
    "            node_name = np.load(load_path +'hasse_%s_maxorder%d.nodename.npz' % (HASSE_TYPE, max_order))['arr_0']\n",
    "\n",
    "            walks_file = load_path + 'n%s_p%s_walks_simplex2vec_%s_maxorder%d.txt.gz'%(str(N), str(P), HASSE_TYPE, max_order)\n",
    "\n",
    "            for EMBDIM in embdim_list:\n",
    "\n",
    "                PARAMS = '%s_%s_%s_%s' %\\\n",
    "                            ( 'dim'+str(EMBDIM), 'n'+str(N), 'p'+str(P), 'walklen'+str(WALKLEN))\n",
    "\n",
    "                save_path = WORK_FOLDER + 'processed-output/embeddings/%s/%s/'%(DATASET, PARAMS)\n",
    "                os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "                save_file = save_path + 's2vembs_%s_%s_maxorder%d.%s.pkl'\\\n",
    "                                          %(SG, HASSE_TYPE, max_order, SEED)\n",
    "                # fit word2vec\n",
    "                sents = LineSentence(walks_file)\n",
    "                model = Word2Vec(sentences=sents, min_count=1, sg=0, \n",
    "                                 size=EMBDIM, window=10, workers=30, seed=SEED)\n",
    "\n",
    "                with open(save_file, 'wb') as fh:\n",
    "                    pkl.dump(dict(zip(node_name[list(map(int, model.wv.index2word))], \n",
    "                                        [_ for _ in model.wv.vectors])), fh, protocol=pkl.HIGHEST_PROTOCOL)\n",
    "                    \n",
    "                print('DONE:', DATASET, HASSE_TYPE, max_order, EMBDIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
