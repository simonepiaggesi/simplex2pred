{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spiaggesi/anaconda3/envs/tf2/lib/python3.6/site-packages/sklearn/utils/deprecation.py:143: FutureWarning: The sklearn.metrics.ranking module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.metrics. Anything that cannot be imported from sklearn.metrics is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASETS =  ['contact-high-school', 'contact-primary-school']\n",
    "SG = 'cbow'\n",
    "HASSE_LIST = ['uniform', 'counts', 'NObias', 'LOexp'] \n",
    "WORK_FOLDER = './'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct reconstruction/prediction test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for DATASET in DATASETS:\n",
    "    \n",
    "    hyperedges_path = WORK_FOLDER + 'processed-output/hyperedges/%s/'%(DATASET)\n",
    "\n",
    "    save_path = WORK_FOLDER + 'processed-output/tables/%s/'%(DATASET)\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "    ##############\n",
    "    \n",
    "    #reconstruction sets\n",
    "    positive_ex = np.load(hyperedges_path+'%s_pos_%s_%dstring.npz'%\\\n",
    "                            ('reconstruction', 'all', 4),\n",
    "                                 allow_pickle=True)['arr_0']\n",
    "    negative_ex = np.load(hyperedges_path+'%s_neg_%s_%dstring.npz'%\\\n",
    "                            ('reconstruction', 'all', 4),\n",
    "                             allow_pickle=True)['arr_0']\n",
    "    negative_bounds = np.load(hyperedges_path+'%s_neg_%s_%dbounds.npz'%\\\n",
    "                            ('reconstruction', 'all', 4),\n",
    "                             allow_pickle=True)['arr_0']\n",
    "\n",
    "    positive_ex = np.array(list(map(lambda x: x.split(','), positive_ex)))\n",
    "    negative_ex = np.array(list(map(lambda x: x.split(','), negative_ex[negative_bounds==4])))\n",
    "\n",
    "    open_train = np.concatenate((positive_ex, negative_ex)).astype(int)\n",
    "\n",
    "    y_train = np.array([1,]*positive_ex.shape[0] + [0,]*negative_ex.shape[0]) \n",
    "\n",
    "    open_train = np.concatenate((open_train, y_train[:, np.newaxis]), axis=1)\n",
    "    np.savez_compressed(save_path+'open-quadruples-0-80.npz', open_train)\n",
    "\n",
    "    ##############\n",
    "    \n",
    "    #prediction sets\n",
    "    positive_ex = np.load(hyperedges_path+'%s_pos_%s_%dstring.npz'%\\\n",
    "                            ('prediction', 'all', 4),\n",
    "                             allow_pickle=True)['arr_0']\n",
    "    positive_bounds = np.load(hyperedges_path+'%s_pos_%s_%dbounds.npz'%\\\n",
    "                            ('prediction', 'all', 4),\n",
    "                             allow_pickle=True)['arr_0']\n",
    "    negative_ex = np.load(hyperedges_path+'%s_neg_%s_%dstring.npz'%\\\n",
    "                            ('prediction', 'all', 4),\n",
    "                             allow_pickle=True)['arr_0']\n",
    "    negative_bounds = np.load(hyperedges_path+'%s_neg_%s_%dbounds.npz'%\\\n",
    "                            ('prediction', 'all', 4),\n",
    "                                 allow_pickle=True)['arr_0']\n",
    "\n",
    "    positive_ex = np.array(list(map(lambda x: x.split(','), positive_ex[positive_bounds==4])))\n",
    "    negative_ex = np.array(list(map(lambda x: x.split(','), negative_ex[negative_bounds==4])))\n",
    "\n",
    "    if positive_ex.shape[0]>0 and negative_ex.shape[0]>0:\n",
    "        open_test = np.concatenate((positive_ex, negative_ex)).astype(int)\n",
    "    elif positive_ex.shape[0]==0:\n",
    "        open_test = negative_ex.astype(int)\n",
    "    else:\n",
    "        open_test = positive_ex.astype(int)\n",
    "\n",
    "    y_test = np.array([1,]*positive_ex.shape[0] + [0,]*negative_ex.shape[0]) \n",
    "\n",
    "    open_test = np.concatenate((open_test, y_test[:, np.newaxis]), axis=1)\n",
    "    np.savez_compressed(save_path+'open-quadruples-80-100.npz', open_test)\n",
    "    \n",
    "    #############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Reconstruction Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = 1.\n",
    "N = 10\n",
    "WALKLEN = 80\n",
    "SEED = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "embdim_list = [8, 16, 32, 64, 128, 256, 512, 1024]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for DATASET in DATASETS:\n",
    "\n",
    "    load_path = WORK_FOLDER + 'processed-output/tables/%s/'%(DATASET)\n",
    "\n",
    "    _, _, data_train, _ = make_train_test_data(DATASET)\n",
    "    proj_g = nx.Graph([tuple(s) for s in data_train if len(s)==2])\n",
    "    nodes_train = sorted(nx.connected_components(proj_g), key=len, reverse=True)[0]\n",
    "\n",
    "    open_test = np.load(load_path + 'open-quadruples-0-80.npz')['arr_0']\n",
    "    tetrads_test = open_test[:,:4].astype(str)\n",
    "    y_test = open_test[:,-1]\n",
    "\n",
    "    assert(np.unique(y_test).shape[0]>1)\n",
    "    \n",
    "    for HASSE_TYPE in HASSE_LIST:\n",
    "\n",
    "        for max_order in range(1, MAX_ORDER+1):\n",
    "            for EMBDIM in embdim_list:\n",
    "\n",
    "                PARAMS = '%s_%s_%s_%s' %\\\n",
    "                                ( 'dim'+str(EMBDIM), 'n'+str(N), 'p'+str(P), 'walklen'+str(WALKLEN))\n",
    "\n",
    "                load_path = WORK_FOLDER + 'processed-output/embeddings/%s/%s/'%(DATASET, PARAMS)\n",
    "\n",
    "                save_path = WORK_FOLDER + 'processed-output/tables/%s/%s/'%(DATASET, PARAMS)\n",
    "                os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "                if os.path.isdir(load_path):\n",
    "\n",
    "                    #Load Embeddings\n",
    "                    with open(load_path+'s2vembs_%s_%s_maxorder%s.%s.pkl'%\\\n",
    "                                (SG, HASSE_TYPE, max_order, SEED), 'rb') as fh:\n",
    "                        model_wv = pkl.load(fh)\n",
    "\n",
    "                    #node embedding\n",
    "                    tf_arrays = map(lambda a: [(model_wv[h], model_wv[k]) for h,k in combinations(a,2)],\n",
    "                                            tetrads_test)\n",
    "\n",
    "                    X_test = np.array(list(map(lambda x: np.mean([a*b for a,b in x], axis=0), tf_arrays)))\n",
    "                    y_pred = X_test.sum(axis=1)\n",
    "                    np.savez_compressed(save_path+'open-quadruples-hadamard-sim-0simplex-0-80-%s-%s-%s-maxorder%s.%s.npz'%\\\n",
    "                                            (SG, 's2v', HASSE_TYPE, max_order, SEED), y_pred)\n",
    "\n",
    "                    #edge embedding\n",
    "                    tf_arrays = map(lambda a: [(model_wv[h], model_wv[k]) for h,k in\n",
    "                            combinations([','.join(map(str, sorted(map(int, edge)))) for edge in combinations(a,2)], 2)],\n",
    "                            tetrads_test)\n",
    "\n",
    "                    X_test = np.array(list(map(lambda x: np.mean([a*b for a,b in x], axis=0), tf_arrays)))\n",
    "                    y_pred = X_test.sum(axis=1)\n",
    "                    np.savez_compressed(save_path+'open-quadruples-hadamard-sim-1simplex-0-80-%s-%s-%s-maxorder%s.%s.npz'%\\\n",
    "                                            (SG, 's2v', HASSE_TYPE, max_order, SEED), y_pred)\n",
    "\n",
    "                    if max_order>1:\n",
    "\n",
    "                        #triangle embedding\n",
    "                        tf_arrays = map(lambda a: [(model_wv[h], model_wv[k]) for h,k in\n",
    "                                combinations([','.join(map(str, sorted(map(int, tris)))) for tris in combinations(a,3)], 2)],\n",
    "                                tetrads_test)\n",
    "\n",
    "                        X_test = np.array(list(map(lambda x: np.mean([a*b for a,b in x], axis=0), tf_arrays)))\n",
    "                        y_pred = X_test.sum(axis=1)\n",
    "                        np.savez_compressed(save_path+'open-quadruples-hadamard-sim-2simplex-0-80-%s-%s-%s-maxorder%s.%s.npz'%\\\n",
    "                                            (SG, 's2v', HASSE_TYPE, max_order, SEED), y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconstruction Search and Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contact-high-school\n",
      "\n",
      "H2: uniform\n",
      "s0 =  65.3 Â± 3.8 (8dims)\n",
      "s1 =  51.2 Â± 3.8 (8dims)\n",
      "s2 =  68.1 Â± 5.1 (8dims)\n",
      "H3: uniform\n",
      "s0 =  67.7 Â± 5.7 (16dims)\n",
      "s1 =  48.2 Â± 4.1 (16dims)\n",
      "s2 =  100.0 Â± 0.1 (16dims)\n",
      "\n",
      "H2: counts\n",
      "s0 =  64.0 Â± 2.7 (8dims)\n",
      "s1 =  49.0 Â± 5.0 (1024dims)\n",
      "s2 =  60.5 Â± 4.4 (1024dims)\n",
      "H3: counts\n",
      "s0 =  59.8 Â± 3.3 (8dims)\n",
      "s1 =  47.0 Â± 5.0 (16dims)\n",
      "s2 =  56.0 Â± 5.2 (16dims)\n",
      "\n",
      "H2: NObias\n",
      "s0 =  53.9 Â± 4.4 (8dims)\n",
      "s1 =  47.7 Â± 4.6 (1024dims)\n",
      "s2 =  58.6 Â± 4.3 (1024dims)\n",
      "H3: NObias\n",
      "s0 =  57.4 Â± 4.4 (8dims)\n",
      "s1 =  47.1 Â± 4.5 (64dims)\n",
      "s2 =  83.3 Â± 3.2 (64dims)\n",
      "\n",
      "H2: LOexp\n",
      "s0 =  55.5 Â± 7.2 (16dims)\n",
      "s1 =  64.7 Â± 4.4 (8dims)\n",
      "s2 =  85.0 Â± 3.6 (8dims)\n",
      "H3: LOexp\n",
      "s0 =  57.7 Â± 7.2 (16dims)\n",
      "s1 =  73.2 Â± 3.3 (8dims)\n",
      "s2 =  90.8 Â± 1.9 (8dims)\n",
      "\n",
      "contact-primary-school\n",
      "\n",
      "H2: uniform\n",
      "s0 =  59.4 Â± 3.7 (16dims)\n",
      "s1 =  43.1 Â± 3.9 (8dims)\n",
      "s2 =  72.1 Â± 3.1 (8dims)\n",
      "H3: uniform\n",
      "s0 =  57.7 Â± 3.3 (1024dims)\n",
      "s1 =  41.7 Â± 3.9 (8dims)\n",
      "s2 =  100.0 Â± 0.0 (8dims)\n",
      "\n",
      "H2: counts\n",
      "s0 =  51.9 Â± 3.9 (8dims)\n",
      "s1 =  46.8 Â± 3.1 (64dims)\n",
      "s2 =  52.9 Â± 3.0 (64dims)\n",
      "H3: counts\n",
      "s0 =  51.4 Â± 4.0 (8dims)\n",
      "s1 =  44.0 Â± 3.1 (1024dims)\n",
      "s2 =  49.3 Â± 3.0 (1024dims)\n",
      "\n",
      "H2: NObias\n",
      "s0 =  47.1 Â± 2.4 (16dims)\n",
      "s1 =  42.7 Â± 2.9 (256dims)\n",
      "s2 =  52.5 Â± 2.9 (256dims)\n",
      "H3: NObias\n",
      "s0 =  43.1 Â± 1.7 (8dims)\n",
      "s1 =  41.4 Â± 2.9 (1024dims)\n",
      "s2 =  85.3 Â± 3.0 (1024dims)\n",
      "\n",
      "H2: LOexp\n",
      "s0 =  50.9 Â± 3.3 (1024dims)\n",
      "s1 =  57.5 Â± 3.4 (8dims)\n",
      "s2 =  70.0 Â± 3.5 (8dims)\n",
      "H3: LOexp\n",
      "s0 =  62.3 Â± 2.9 (16dims)\n",
      "s1 =  72.4 Â± 3.5 (8dims)\n",
      "s2 =  90.5 Â± 1.1 (8dims)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for DATASET in DATASETS:\n",
    "    \n",
    "    load_path = WORK_FOLDER + 'processed-output/tables/%s/'%(DATASET)\n",
    "\n",
    "    y_test = np.load(load_path+'open-quadruples-0-80.npz')['arr_0'][:,-1]\n",
    "    random_baseline = y_test.sum()/len(y_test)\n",
    "    \n",
    "    print(DATASET)\n",
    "    \n",
    "    print()\n",
    "\n",
    "    params_folders = glob.glob(load_path+'dim*')\n",
    "    \n",
    "    for HASSE_TYPE in HASSE_LIST:\n",
    "\n",
    "        for max_order in range(2, MAX_ORDER+1):\n",
    "\n",
    "            print('H'+str(max_order)+':', HASSE_TYPE)\n",
    "\n",
    "            scores_simplex0 = []\n",
    "            scores_simplex1 = []\n",
    "            scores_simplex2 = []\n",
    "            for f in params_folders:\n",
    "                PARAMS = f.split('/')[-1]\n",
    "                EMBDIM = PARAMS.split('_')[0].replace('dim', '')\n",
    "\n",
    "                y_pred = np.load(f + '/open-quadruples-hadamard-sim-0simplex-0-80-%s-%s-%s-maxorder%s.%s.npz'%\\\n",
    "                                (SG, 's2v', HASSE_TYPE, max_order, 0))['arr_0']\n",
    "                scores_simplex0.append((classification_score_from_y4(y_test, y_pred), EMBDIM+'dims'))\n",
    "\n",
    "                y_pred = np.load(f + '/open-quadruples-hadamard-sim-1simplex-0-80-%s-%s-%s-maxorder%s.%s.npz'%\\\n",
    "                                (SG, 's2v', HASSE_TYPE, max_order, 0))['arr_0']\n",
    "                scores_simplex1.append((classification_score_from_y4(y_test, y_pred), EMBDIM+'dims'))\n",
    "\n",
    "                if max_order>1:\n",
    "                    y_pred = np.load(f + '/open-quadruples-hadamard-sim-2simplex-0-80-%s-%s-%s-maxorder%s.%s.npz'%\\\n",
    "                                (SG, 's2v', HASSE_TYPE, max_order, 0))['arr_0']\n",
    "                    scores_simplex2.append((classification_score_from_y4(y_test, y_pred), EMBDIM+'dims'))\n",
    "\n",
    "            scores_simplex0 = sorted(scores_simplex0, reverse=True)\n",
    "            scores_simplex1 = sorted(scores_simplex1, reverse=True)\n",
    "            scores_simplex2 = sorted(scores_simplex2, reverse=True)\n",
    "            print('s0 = ', round(scores_simplex0[0][0][0]*100,1), u\"\\u00B1\", round(scores_simplex0[0][0][1]*100,1), '('+scores_simplex0[0][1]+')') \n",
    "            print('s1 = ',round(scores_simplex1[0][0][0]*100,1), u\"\\u00B1\", round(scores_simplex1[0][0][1]*100,1), '('+scores_simplex1[0][1]+')') \n",
    "            print('s2 = ',round(scores_simplex2[0][0][0]*100,1), u\"\\u00B1\", round(scores_simplex2[0][0][1]*100,1), '('+scores_simplex1[0][1]+')')\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Prediction Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for DATASET in DATASETS:\n",
    "\n",
    "    load_path = WORK_FOLDER + 'processed-output/tables/%s/'%(DATASET)\n",
    "\n",
    "    _, _, data_train, _ = make_train_test_data(DATASET)\n",
    "    proj_g = nx.Graph([tuple(s) for s in data_train if len(s)==2])\n",
    "    nodes_train = sorted(nx.connected_components(proj_g), key=len, reverse=True)[0]\n",
    "\n",
    "    open_test = np.load(load_path + 'open-quadruples-80-100.npz')['arr_0']\n",
    "    tetrads_test = open_test[:,:4].astype(str)\n",
    "    y_test = open_test[:,-1]\n",
    "\n",
    "    assert(np.unique(y_test).shape[0]>1)\n",
    "    \n",
    "    for HASSE_TYPE in HASSE_LIST:\n",
    "\n",
    "        for max_order in range(1, MAX_ORDER+1):\n",
    "            for EMBDIM in embdim_list:\n",
    "\n",
    "                PARAMS = '%s_%s_%s_%s' %\\\n",
    "                                ( 'dim'+str(EMBDIM), 'n'+str(N), 'p'+str(P), 'walklen'+str(WALKLEN))\n",
    "\n",
    "                load_path = WORK_FOLDER + 'processed-output/embeddings/%s/%s/'%(DATASET, PARAMS)\n",
    "\n",
    "                save_path = WORK_FOLDER + 'processed-output/tables/%s/%s/'%(DATASET, PARAMS)\n",
    "                os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "                if os.path.isdir(load_path):\n",
    "\n",
    "                    #Load Embeddings\n",
    "                    with open(load_path+'s2vembs_%s_%s_maxorder%s.%s.pkl'%\\\n",
    "                                (SG, HASSE_TYPE, max_order, SEED), 'rb') as fh:\n",
    "                        model_wv = pkl.load(fh)\n",
    "\n",
    "                    #node embedding\n",
    "                    tf_arrays = map(lambda a: [(model_wv[h], model_wv[k]) for h,k in combinations(a,2)],\n",
    "                                            tetrads_test)\n",
    "\n",
    "                    X_test = np.array(list(map(lambda x: np.mean([a*b for a,b in x], axis=0), tf_arrays)))\n",
    "                    y_pred = X_test.sum(axis=1)\n",
    "                    np.savez_compressed(save_path+'open-quadruples-hadamard-sim-0simplex-80-100-%s-%s-%s-maxorder%s.%s.npz'%\\\n",
    "                                            (SG, 's2v', HASSE_TYPE, max_order, SEED), y_pred)\n",
    "\n",
    "                    #edge embedding\n",
    "                    tf_arrays = map(lambda a: [(model_wv[h], model_wv[k]) for h,k in\n",
    "                            combinations([','.join(map(str, sorted(map(int, edge)))) for edge in combinations(a,2)], 2)],\n",
    "                            tetrads_test)\n",
    "\n",
    "                    X_test = np.array(list(map(lambda x: np.mean([a*b for a,b in x], axis=0), tf_arrays)))\n",
    "                    y_pred = X_test.sum(axis=1)\n",
    "                    np.savez_compressed(save_path+'open-quadruples-hadamard-sim-1simplex-80-100-%s-%s-%s-maxorder%s.%s.npz'%\\\n",
    "                                            (SG, 's2v', HASSE_TYPE, max_order, SEED), y_pred)\n",
    "\n",
    "                    if max_order>1:\n",
    "\n",
    "                        #triangle embedding\n",
    "                        tf_arrays = map(lambda a: [(model_wv[h], model_wv[k]) for h,k in\n",
    "                                combinations([','.join(map(str, sorted(map(int, tris)))) for tris in combinations(a,3)], 2)],\n",
    "                                tetrads_test)\n",
    "\n",
    "                        X_test = np.array(list(map(lambda x: np.mean([a*b for a,b in x], axis=0), tf_arrays)))\n",
    "                        y_pred = X_test.sum(axis=1)\n",
    "                        np.savez_compressed(save_path+'open-quadruples-hadamard-sim-2simplex-80-100-%s-%s-%s-maxorder%s.%s.npz'%\\\n",
    "                                            (SG, 's2v', HASSE_TYPE, max_order, SEED), y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction Search and Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contact-high-school\n",
      "\n",
      "H2: uniform\n",
      "s0 =  68.5 Â± 17.3 (512dims)\n",
      "s1 =  88.8 Â± 13.0 (64dims)\n",
      "s2 =  55.8 Â± 11.5 (64dims)\n",
      "H3: uniform\n",
      "s0 =  77.5 Â± 14.9 (1024dims)\n",
      "s1 =  85.7 Â± 12.2 (128dims)\n",
      "s2 =  52.6 Â± 10.1 (128dims)\n",
      "\n",
      "H2: counts\n",
      "s0 =  92.2 Â± 9.8 (8dims)\n",
      "s1 =  96.1 Â± 4.6 (1024dims)\n",
      "s2 =  89.2 Â± 6.6 (1024dims)\n",
      "H3: counts\n",
      "s0 =  82.5 Â± 10.4 (32dims)\n",
      "s1 =  96.1 Â± 4.6 (64dims)\n",
      "s2 =  89.5 Â± 12.1 (64dims)\n",
      "\n",
      "H2: NObias\n",
      "s0 =  91.0 Â± 5.6 (512dims)\n",
      "s1 =  91.0 Â± 5.6 (64dims)\n",
      "s2 =  74.3 Â± 9.8 (64dims)\n",
      "H3: NObias\n",
      "s0 =  91.0 Â± 5.6 (128dims)\n",
      "s1 =  91.0 Â± 5.6 (64dims)\n",
      "s2 =  72.5 Â± 9.8 (64dims)\n",
      "\n",
      "H2: LOexp\n",
      "s0 =  89.5 Â± 12.1 (32dims)\n",
      "s1 =  81.6 Â± 15.0 (256dims)\n",
      "s2 =  49.2 Â± 6.4 (256dims)\n",
      "H3: LOexp\n",
      "s0 =  83.6 Â± 12.2 (64dims)\n",
      "s1 =  69.8 Â± 13.9 (512dims)\n",
      "s2 =  76.2 Â± 15.1 (512dims)\n",
      "\n",
      "contact-primary-school\n",
      "\n",
      "H2: uniform\n",
      "s0 =  65.3 Â± 6.7 (16dims)\n",
      "s1 =  59.5 Â± 4.0 (16dims)\n",
      "s2 =  83.3 Â± 0.0 (16dims)\n",
      "H3: uniform\n",
      "s0 =  57.5 Â± 8.1 (16dims)\n",
      "s1 =  62.0 Â± 4.2 (8dims)\n",
      "s2 =  60.9 Â± 9.1 (8dims)\n",
      "\n",
      "H2: counts\n",
      "s0 =  66.5 Â± 9.4 (128dims)\n",
      "s1 =  66.5 Â± 2.9 (256dims)\n",
      "s2 =  69.9 Â± 5.2 (256dims)\n",
      "H3: counts\n",
      "s0 =  63.5 Â± 9.5 (1024dims)\n",
      "s1 =  67.7 Â± 3.1 (512dims)\n",
      "s2 =  70.3 Â± 5.7 (512dims)\n",
      "\n",
      "H2: NObias\n",
      "s0 =  57.0 Â± 6.8 (16dims)\n",
      "s1 =  65.6 Â± 1.3 (32dims)\n",
      "s2 =  68.8 Â± 7.7 (32dims)\n",
      "H3: NObias\n",
      "s0 =  65.4 Â± 2.8 (32dims)\n",
      "s1 =  66.3 Â± 1.5 (32dims)\n",
      "s2 =  67.2 Â± 6.1 (32dims)\n",
      "\n",
      "H2: LOexp\n",
      "s0 =  59.4 Â± 8.1 (8dims)\n",
      "s1 =  60.3 Â± 6.4 (64dims)\n",
      "s2 =  89.2 Â± 8.8 (64dims)\n",
      "H3: LOexp\n",
      "s0 =  55.7 Â± 5.3 (64dims)\n",
      "s1 =  54.2 Â± 6.9 (32dims)\n",
      "s2 =  74.0 Â± 11.5 (32dims)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for DATASET in DATASETS:\n",
    "\n",
    "    load_path = WORK_FOLDER + 'processed-output/tables/%s/'%(DATASET)\n",
    "\n",
    "    y_test = np.load(load_path+'open-quadruples-80-100.npz')['arr_0'][:,-1]\n",
    "    random_baseline = y_test.sum()/len(y_test)\n",
    "    \n",
    "    print(DATASET)\n",
    "    \n",
    "    print()\n",
    "\n",
    "    params_folders = glob.glob(load_path+'dim*')\n",
    "    \n",
    "    for HASSE_TYPE in HASSE_LIST:\n",
    "\n",
    "        for max_order in range(2, MAX_ORDER+1):\n",
    "\n",
    "            print('H'+str(max_order)+':', HASSE_TYPE)\n",
    "\n",
    "            scores_simplex0 = []\n",
    "            scores_simplex1 = []\n",
    "            scores_simplex2 = []\n",
    "            for f in params_folders:\n",
    "                PARAMS = f.split('/')[-1]\n",
    "                EMBDIM = PARAMS.split('_')[0].replace('dim', '')\n",
    "\n",
    "                y_pred = np.load(f + '/open-quadruples-hadamard-sim-0simplex-80-100-%s-%s-%s-maxorder%s.%s.npz'%\\\n",
    "                                (SG, 's2v', HASSE_TYPE, max_order, 0))['arr_0']\n",
    "                scores_simplex0.append((classification_score_from_y4(y_test, y_pred), EMBDIM+'dims'))\n",
    "\n",
    "                y_pred = np.load(f + '/open-quadruples-hadamard-sim-1simplex-80-100-%s-%s-%s-maxorder%s.%s.npz'%\\\n",
    "                                (SG, 's2v', HASSE_TYPE, max_order, 0))['arr_0']\n",
    "                scores_simplex1.append((classification_score_from_y4(y_test, y_pred), EMBDIM+'dims'))\n",
    "\n",
    "                if max_order>1:\n",
    "                    y_pred = np.load(f + '/open-quadruples-hadamard-sim-2simplex-80-100-%s-%s-%s-maxorder%s.%s.npz'%\\\n",
    "                                (SG, 's2v', HASSE_TYPE, max_order, 0))['arr_0']\n",
    "                    scores_simplex2.append((classification_score_from_y4(y_test, y_pred), EMBDIM+'dims'))\n",
    "\n",
    "            scores_simplex0 = sorted(scores_simplex0, reverse=True)\n",
    "            scores_simplex1 = sorted(scores_simplex1, reverse=True)\n",
    "            scores_simplex2 = sorted(scores_simplex2, reverse=True)\n",
    "            print('s0 = ', round(scores_simplex0[0][0][0]*100,1), u\"\\u00B1\", round(scores_simplex0[0][0][1]*100,1), '('+scores_simplex0[0][1]+')') \n",
    "            print('s1 = ',round(scores_simplex1[0][0][0]*100,1), u\"\\u00B1\", round(scores_simplex1[0][0][1]*100,1), '('+scores_simplex1[0][1]+')') \n",
    "            print('s2 = ',round(scores_simplex2[0][0][0]*100,1), u\"\\u00B1\", round(scores_simplex2[0][0][1]*100,1), '('+scores_simplex1[0][1]+')')\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
