{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import networkx as nx\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASETS = ['contact-high-school', 'contact-primary-school']\n",
    "WORK_FOLDER = './'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_SIZE = int(1e7)\n",
    "TUPLE_SIZE = 3 #,4 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample Node Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "\n",
    "for DATASET in DATASETS:\n",
    "    \n",
    "    #Load Simplices Data\n",
    "    _, _, data_train, _ = make_train_test_data(DATASET)\n",
    "\n",
    "    proj_g = nx.Graph([tuple(s) for s in data_train if len(s)==2])\n",
    "    max_comp = sorted(nx.connected_components(proj_g), key=len, reverse=True)[0]\n",
    "    proj_g = nx.subgraph(proj_g, max_comp)\n",
    "\n",
    "    nodes_train = set(proj_g.nodes())\n",
    "    adj_dict = nx.convert.to_dict_of_lists(proj_g)\n",
    "\n",
    "    data_train = [s for s in data_train if s.issubset(nodes_train)]\n",
    "\n",
    "    save_path = WORK_FOLDER + 'processed-output/hyperedges/%s/'%(DATASET)\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "    #NAIVE RANDOM NEGATIVE SAMPLING\n",
    "    random_ = naive_negative_sampling(list(nodes_train), sample_size=SAMPLE_SIZE, tuple_size=TUPLE_SIZE)\n",
    "    np.savez_compressed(save_path + 'negative_random_%dtuple.npz'%TUPLE_SIZE, random_)\n",
    "    random_ = []\n",
    "    print('DONE:', DATASET, TUPLE_SIZE, 'indep. sampling')\n",
    "\n",
    "    #MOTIFS NEGATIVE SAMPLING\n",
    "    motifs_ = motifs_negative_sampling(list(nodes_train), adj_dict, sample_size=SAMPLE_SIZE, tuple_size=TUPLE_SIZE)\n",
    "    np.savez_compressed(save_path + 'negative_motifs_%dtuple.npz'%TUPLE_SIZE, motifs_) \n",
    "    motifs_ = []\n",
    "    print('DONE:', DATASET, TUPLE_SIZE, 'motifs sampling')\n",
    "\n",
    "    #STARS NEGATIVE SAMPLING\n",
    "    stars_ = stars_negative_sampling(list(nodes_train), adj_dict, sample_size=SAMPLE_SIZE, tuple_size=TUPLE_SIZE)\n",
    "    np.savez_compressed(save_path + 'negative_stars_%dtuple.npz'%TUPLE_SIZE, stars_)\n",
    "    stars_ = []\n",
    "    print('DONE:', DATASET, TUPLE_SIZE, 'stars sampling')\n",
    "\n",
    "    #CLIQUES NEGATIVE SAMPLING\n",
    "    cliques_ = cliques_negative_sampling(list(data_train), adj_dict, sample_size=SAMPLE_SIZE, tuple_size=TUPLE_SIZE)\n",
    "    np.savez_compressed(save_path + 'negative_cliques_%dtuple.npz'% TUPLE_SIZE, cliques_)\n",
    "    cliques_ = []\n",
    "    print('DONE:', DATASET, TUPLE_SIZE, 'cliques sampling')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct Positive/Negative Examples for Classification Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for DATASET in DATASETS:\n",
    "    \n",
    "    #Load Simplices Data\n",
    "    _, _, data_train, data_test = make_train_test_data(DATASET)\n",
    "\n",
    "    proj_g = nx.Graph([tuple(s) for s in data_train if len(s)==2])\n",
    "    max_comp = sorted(nx.connected_components(proj_g), key=len, reverse=True)[0]\n",
    "    proj_g = nx.subgraph(proj_g, max_comp)\n",
    "\n",
    "    nodes_train = set(proj_g.nodes())\n",
    "\n",
    "    data_train = [s for s in data_train if s.issubset(nodes_train)]\n",
    "    data_test = [s for s in data_test if s.issubset(nodes_train)]\n",
    "\n",
    "    #map from frozensets to strings\n",
    "    train_simplices = np.array(list(map(lambda u: ','.join(map(str, sorted(map(int, u)))), data_train)), dtype=object)\n",
    "    train_sizes = np.array(list(map(lambda u: len(u.split(',')), train_simplices)))\n",
    "    test_simplices = np.array(list(map(lambda u: ','.join(map(str, sorted(map(int, u)))), data_test)), dtype=object)\n",
    "    test_sizes = np.array(list(map(lambda u: len(u.split(',')), test_simplices)))\n",
    "\n",
    "    save_path = WORK_FOLDER + 'processed-output/hyperedges/%s/'%(DATASET)\n",
    "    load_path = save_path \n",
    "    os.makedirs(save_path, exist_ok=True)    \n",
    "\n",
    "    neg_tuple = set()\n",
    "    for neg_sample in ['random', 'stars', 'motifs', 'cliques']:\n",
    "        #load negative samples\n",
    "        neg_tuple = neg_tuple | set(np.load(load_path + 'negative_%s_%dtuple.npz'%\\\n",
    "                               (neg_sample, TUPLE_SIZE), allow_pickle=True)['arr_0'])\n",
    "\n",
    "########################\n",
    "        \n",
    "    #reconstruction\n",
    "    pos_ = train_simplices[train_sizes==TUPLE_SIZE]\n",
    "    neg_ = np.array(list(neg_tuple - set(train_simplices[train_sizes==TUPLE_SIZE])), dtype=object)\n",
    "\n",
    "    np.savez_compressed(save_path + '%s_pos_%s_%dstring.npz'%\\\n",
    "        ('reconstruction', 'all', TUPLE_SIZE), pos_)\n",
    "    np.savez_compressed(save_path + '%s_neg_%s_%dstring.npz'%\\\n",
    "        ('reconstruction', 'all', TUPLE_SIZE), neg_)\n",
    "\n",
    "    train_boundary = set(train_simplices[train_sizes==TUPLE_SIZE-1])\n",
    "\n",
    "    negative_boundaries = np.array(list(map(lambda x: np.sum([','.join(map(str, sorted(map(int, i)))) in train_boundary \n",
    "                                                for i in combinations(x.split(','), TUPLE_SIZE-1)]), neg_)))\n",
    "    np.savez_compressed(save_path + '%s_neg_%s_%dbounds.npz'%\\\n",
    "        ('reconstruction', 'all', TUPLE_SIZE), negative_boundaries)\n",
    "    \n",
    "    print('DONE:', DATASET, TUPLE_SIZE, 'reconstruction examples')\n",
    "    \n",
    "########################\n",
    "\n",
    "    #prediction\n",
    "    pos_ = np.array(list(set(test_simplices[test_sizes==TUPLE_SIZE]) \n",
    "                  - set(train_simplices[train_sizes==TUPLE_SIZE])), dtype=object)\n",
    "    neg_ = np.array(list(neg_tuple - \n",
    "                    (set(train_simplices[train_sizes==TUPLE_SIZE]) | set(test_simplices[test_sizes==TUPLE_SIZE]))\n",
    "                      ), dtype=object)\n",
    "\n",
    "    np.savez_compressed(save_path + '%s_pos_%s_%dstring.npz'%\\\n",
    "        ('prediction', 'all', TUPLE_SIZE), pos_)\n",
    "    np.savez_compressed(save_path + '%s_neg_%s_%dstring.npz'%\\\n",
    "        ('prediction', 'all', TUPLE_SIZE), neg_)\n",
    "\n",
    "    train_boundary = set(train_simplices[train_sizes==TUPLE_SIZE-1])\n",
    "\n",
    "    positive_boundaries = np.array(list(map(lambda x: np.sum([','.join(map(str, sorted(map(int, i)))) in train_boundary \n",
    "                                for i in combinations(x.split(','), TUPLE_SIZE-1)]), pos_)))\n",
    "    negative_boundaries = np.array(list(map(lambda x: np.sum([','.join(map(str, sorted(map(int, i)))) in train_boundary \n",
    "                                for i in combinations(x.split(','), TUPLE_SIZE-1)]), neg_)))\n",
    "\n",
    "    np.savez_compressed(save_path + '%s_pos_%s_%dbounds.npz'%\\\n",
    "        ('prediction', 'all', TUPLE_SIZE), positive_boundaries)\n",
    "    np.savez_compressed(save_path + '%s_neg_%s_%dbounds.npz'%\\\n",
    "        ('prediction', 'all', TUPLE_SIZE), negative_boundaries)\n",
    "    \n",
    "    print('DONE:', DATASET, TUPLE_SIZE, 'prediction examples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
